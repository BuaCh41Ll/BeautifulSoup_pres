# Liberia election results

import sys
sys.path.append('/usr/lib/python2.7/dist-packages/')
from bs4 import BeautifulSoup

import urllib2
import csv
import os
import re

os.chdir("/home/andrew/Dropbox/python/BeautifulSoup_pres/")

########   Pres/VP   #####################
f= csv.writer(open("presidential_res.csv", "w"))
f.writerow(["PresVPcand","lastname","party","votes","county","precinct"]) # Write column headers as the first line

#orig
page1 = urllib2.urlopen("http://www.necliberia.org/results2011/pp_results/03002.html")
page2 = urllib2.urlopen("http://www.necliberia.org/results2011/pp_results/09006.html")

pages = [page1, page2]

for page in pages:
	soup = BeautifulSoup(page)

	# h2 - has county
	h2 = soup.find_all("h2")
	county = str(h2[0].get_text())

	#h4 has voting precinct.
	h4 = soup.find_all("h4")
	precinct = str(h4[0].get_text())

	# get the results tables
	res = soup.findAll('div', {'class': 'res'})

	pres = res[0]

	for row in pres.find_all("tr"):
		tds = row.find_all("td")
		try:
			a = str(tds[0].get_text())
			b = str(tds[1].get_text())

		except:
			print "bad string"
			continue

		m = re.search(r'\(([A-Z]+)\)', a)
		if m:
			party = m.group(1)

		else:
			party = ''
		
		m = re.search(r'([A-Z]+),', a)
		if m:
			lastname = m.group(1)
		else:
			lastname = ''

		f.writerow([a, lastname, party, b, county, precinct])

print lastname

### re
# find only first: re.search(pattern, string, flags=0)

#m = re.match(r"(\w+) (\w+)", "Isaac Newton, physicist")
ex1 = "BEYAN, Gladys G. Y. (GDPL)"
ex2 = "Total Valid"



#### just lastname: (all caps, end at comma)
m = re.search(r'([A-Z]+),', ex1)
if m:
	lastname = m.group(1)

### just get party: (in parenthesis, all caps)
m = re.search(r'\(([A-Z]+)\)', ex1)

if m:
   party = m.group(1)
##

	f.writerow([a, party, b])


############### don't use: doesn't work   #############
### all at once
m = re.search(r'([A-Z]+)([\w\s]+)\(([A-Z]+)\)', ex1)

if m:
	lastname = m.group(1)
	party = m.group(3)

########################################################






########   Pres/VP   #####################
f= csv.writer(open("prez.csv", "w"))
f.writerow(["PresVPcand", "votes"]) # Write column headers as the first line

pres = res[0]

for row in pres.find_all("tr"):
	tds = row.find_all("td")
	try:
		a = str(tds[0].get_text())
		b = str(tds[1].get_text())

	except:
		print "bad string"
		continue

	f.writerow([a, b])




####  House  ####################################
f= csv.writer(open("house.csv", "w"))
f.writerow(["HouseCandidate", "votes"]) # Write column headers as the first line

house = res[2]

for row in house.find_all("tr"):
	tds = row.find_all("td")
	try:
		a = str(tds[0].get_text())
		b = str(tds[1].get_text())

	except:
		print "bad string"
		continue

	f.writerow([a, b])





########   Senate   #####################
f= csv.writer(open("house.csv", "w"))
f.writerow(["HouseCandidate", "votes"]) # Write column headers as the first line

sen = res[1]

for row in sen.find_all("tr"):
	tds = row.find_all("td")
	try:
		a = str(tds[0].get_text())
		b = str(tds[1].get_text())
	#	c = str(tds[2].get_text())
	#	d = str(tds[3].get_text())
	except:
		print "bad string"
		continue

	f.writerow([a, b])

##########################

for res in soup.findAll('div', {'class': 'res'}):
	for row in res.find_all("tr"):
		tds = row.find_all("td")
		try:
			a = str(tds[0].get_text())
			b = str(tds[1].get_text())
			c = str(tds[2].get_text())
			d = str(tds[3].get_text())
		except:
			print "bad string"
			continue

		print ([a, b, c, d])
		f.writerow([a, b, c, d])



######################################################################
####################################################################
# approach 2

tbls = soup.find_all('table')
len(tbls)

tables = soup.find_all('table')

len(tables)

# look at:
tables[0].get_text()
tables[2].get_text()
tables[3].get_text()
tables[4].get_text()
tables[5].get_text()


# useful tables are 0, 2, 4, 6-9 as content and 5 as header


res = tbls[-2]


t0 = tables[0]
rows = t0.find_all("tr") #find all of the table rows

for row in rows:
	tds = row.find_all("td")

	try:
		a = str(tds[0].get_text())
		b = str(tds[1].get_text())
		c = str(tds[2].get_text())
		d = str(tds[3].get_text())
	except:
		print "bad string"
		continue

	print ([a, b, c, d])
	f.writerow([a, b, c, d])




###################################################

#rogue = soup.find(width="950")
#rogue.decompose()

#dat = [ map(str, row.findAll("td")) for row in res.findAll("tr") ]







f.writerow(["a", "b", "c", "d"])	# Write column headers as the first line

###### decompose unneeded tables (no unique identifying chars)

final_link = soup.p.a
final_link.decompose()

rows = soup.find_all("tr") #find all of the table rows

for row in rows:
	tds = row.find_all("td")

	try: #we are using "try" because the table is not well formatted. This allows the program to continue after encountering an error.
		a = str(tds[0].get_text())
		b = str(tds[1].get_text())
		c = str(tds[2].get_text())
		d = str(tds[3].get_text())
#		e = tds[4].get_text()
#		f = tds[5].get_text()
#		g = tds[6].get_text()
#		h = tds[7].get_text()
#		i = tds[8].get_text()
	except:
		print "bad string"
		continue

	print ([a, b, c, d])
	f.writerow([a, b, c, d])



###################################################################
width="100%"

rows = soup.find_all("tr") #find all of the table rows

for row in rows:
	tds = row.find_all("td")
 
	try: #we are using "try" because the table is not well formatted. This allows the program to continue after encountering an error.
		first = str(tds[0].get_text()) # This structure isolate the item by its column in the table and converts it into a string.
		second = str(tds[1].get_text())
		third = str(tds[2].get_text())
		fourth = str(tds[3].get_text())
		fifth = str(tds[4].get_text())
		sixth = tds[5].get_text()

	except:
		print "bad string"
		continue


for tr in trs:
        for link in tr.find_all('a'):
#this is a bit tricky - you are combining the search for anchor tags and the for loop in one step
		fullLink = link.get('href') #get the value of the href
 
	tds = tr.find_all("td") #run another search for all of the table data
 
	try: #we are using "try" because the table is not well formatted. This allows the program to continue after encountering an error.
		names = str(tds[0].get_text()) # This structure isolate the item by its column in the table and converts it into a string.
		years = str(tds[1].get_text())
		positions = str(tds[2].get_text())
		parties = str(tds[3].get_text())
		states = str(tds[4].get_text())
		congress = tds[5].get_text() 
		
 
	except:
		print "bad tr string"
		continue #This tells the computer to move on to the next item after it encounters an error
 
	f.writerow([names, years, positions, parties, states, congress, fullLink])
